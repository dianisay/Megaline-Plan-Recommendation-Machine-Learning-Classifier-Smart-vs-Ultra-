# ğŸ“± Smart vs Ultra: Megaline Plan Recommendation Model

## ğŸ“Œ Introduction
Build a supervised classification model to **recommend the optimal Megaline plan (Smart vs Ultra)** from monthly usage behavior.  
Primary KPI: **Accuracy â‰¥ 0.75** on the held-out test set. Secondary checks include class-wise precision/recall and confusion matrix to ensure practical reliability.

---

## ğŸ¯ Business goals
- **Plan assignment:** Predict the right plan to migrate users off legacy plans.
- **Operational impact:** Improve targeting for plan recommendations and reduce misclassification costs.
- **Model quality bar:** Meet or exceed **0.75 accuracy** on unseen data.

---

## ğŸ“‚ Dataset
Each row represents one userâ€™s monthly behavior:

- `calls` â€” number of calls  
- `minutes` â€” total call minutes  
- `messages` â€” number of SMS  
- `mb_used` â€” internet traffic in MB  
- `is_ultra` â€” target label (Ultra=1, Smart=0)

Source file: `datasets/users_behavior.csv`

---

## ğŸ§­ Methodology

### 1) Data loading & checks
- **Schema & missing values:** Validate dtypes, nulls, and outliers.
- **Target distribution:** Inspect class balance (Smart vs Ultra) for proper evaluation.

### 2) Splits & baseline
- **Stratified split:** Train/Validation/Test (e.g., 60/20/20) with fixed `random_state`.
- **Baseline:** `DummyClassifier` (most_frequent and stratified) to set a floor for accuracy.

### 3) Modeling & tuning
- **Feature prep:** Keep raw features; **scale** only for distance/linear models.
- **Candidates:** Logistic Regression, k-NN, Decision Tree, Random Forest, Gradient Boosting.
- **Hyperparameters:** Grid/Random search on the validation set (no test leakage).  
  - Examples:  
    - Logistic: `C`, `penalty`  
    - k-NN: `n_neighbors`, `weights`  
    - Tree: `max_depth`, `min_samples_split`  
    - RandomForest: `n_estimators`, `max_depth`, `max_features`  
    - GradientBoosting: `n_estimators`, `learning_rate`, `max_depth`
- **Selection:** Pick the best validation accuracy; optionally refit on Train+Val before Test.

### 4) Evaluation
- **Primary:** Accuracy on the hold-out test set (target â‰¥ 0.75).
- **Diagnostics:** Confusion matrix; class precision/recall; ROC-AUC as a stability check.
- **Interpretability:** Feature importance (trees/boosting) or coefficients (logistic).

### 5) Sanity checks
- **Leakage guard:** Confirm no target leakage and scaler fit only on training folds.
- **Permutation test:** Shuffle labels to verify accuracy collapses to baseline.
- **Robustness:** Repeat with multiple seeds; report mean Â± std of accuracy.

---

## ğŸ“Š Key analyses & visuals
- **EDA snapshots:** Distributions and correlations among `minutes`, `mb_used`, `calls`, `messages`.
- **Model comparison:** Validation accuracy by model/hyperparameters.
- **Final quality:** Test confusion matrix and per-class metrics; feature importance bar chart.

---

## âœ… Results summary
- **Best model:** <model_name> with tuned hyperparameters.  
- **Test accuracy:** <value> (â‰¥ 0.75 target).  
- **Insights:** Which behaviors most strongly separate Ultra vs Smart (e.g., high `mb_used` or `minutes`).  
- **Next steps:** Threshold tuning by business cost, calibration if probabilities are used.

---

## ğŸ§° Tech stack
- **Python:** pandas, numpy, scikit-learn, matplotlib, seaborn
- **Utilities:** joblib (model persistence), mlxtend (optional for plots)

---

## ğŸš€ Repro steps
1. **Clone & env:** `pip install -r requirements.txt`
2. **Data:** Place `users_behavior.csv` in `datasets/`.
3. **Run:** Execute the notebook or `python src/train.py`
4. **Outputs:** Metrics in `reports/`, figures in `figures/`, model in `models/`

---

## ğŸ“ˆ Sample figures to include
- Validation accuracy per model  
- Test confusion matrix  
- Feature importance (tree-based) or coefficients (logistic)

---

## ğŸ¤ Contact
Created by **Diana <Last Name>**  
ğŸ”— [LinkedIn](https://linkedin.com/in/tuusuario) Â· ğŸŒ [Portfolio](https://tuportfolio.com)
